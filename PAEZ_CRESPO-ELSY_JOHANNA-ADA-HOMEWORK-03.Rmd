---
title: "PAEZ_CRESPO-ELSY_JOHANNA-ADA-HOMEWORK-03"
author: "Elsy Johanna Paez Crespo"
date: "20/3/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<span style="color:blue">***PROBLEM-01:***</span>

Write a simple R function you call Z.prop.test(), that can perform one- or two-sample Z-tests for proportion data, using the following guidelines.

- Your function should take the following arguments: p1 and n1 (no default) representing the estimated proportion and sample size (e.g., based on your sample data); p2 and n2 (both defaulting to NULL) that contain a second sample’s proportion and sample size data in the event of a two-sample test; p0 (no default) as the expected value for the population proportion; and alternative (default “two.sided”) and conf.level (default 0.95), to be used in the same way as in the function t.test().
- When conducting a two-sample test, it should be p1 that is tested as being smaller or larger than p2 when alternative=“less” or alternative=“greater”, respectively, the same as in the use of x and y in the function t.test().
- The function should perform a one-sample Z-test using p1, n1, and p0 if either p2 or n2 (or both) is NULL.
- The function should contain a check for the rules of thumb we have talked about (n×π>5 and n×(1−π)>5) to ensure the validity of assuming the normal distribution in both the one- and two-sample settings. If this is violated, the function should still complete, but it should also print an appropriate warning message.
- The function should return a list containing the following elements: Z (the test statistic), P (the appropriate p value), and CI (the two-sided CI with respect to “conf.level” around p1 in the case of a one-sample test and around p2-p1 in the case of a two-sample test). For all test alternatives (“two.sided”, “greater”, “less”), calculate symmetric CIs based on quantiles of the normal distribution rather than worrying about calculating single-limit confidence bounds.

####FUNCTION CODE:

```{r}
z.prop.test <- function(p1, p2=NULL, n1, n2=NULL, p0=0, alternative = "two.sided", conf.level = 0.95){
  if((n1*p1)>5 && (n1*(1-p1)) > 5){
    message("Validation: Be careful! The assumption of normal distribution is violated!")
    } else{
      message("Validation: Data are normally distributed!")
    }
    if(is.null(p2) | is.null(n2)){
    p1 <- mean(v1)
    n1 <- length(v1)
    p0 <- # must to be defined based on previous data (expected proportion)
    z <- (p1 - p0)/sqrt(p0 * (1 - p0) / n1)
    z
    } else {
    p1 <- mean(v1)
    p2 <- mean(v2)
    n1 <- length(v1)
    n2 <- length(v2)
    pstar <- (sum(v1) + sum(v2)) / (length(v1) + length(v2))
    z <- (p2 - p1)/sqrt((pstar * (1 - pstar)) * (1/length(v1) + 1/length(v2)))
    z
    }
  if(alternative == "two.sided"| is.null(alternative)){
    p.upper <- 1 - pnorm(z, lower.tail = TRUE) 
    p.lower <- pnorm(z, lower.tail = FALSE)
    p <- p.upper + p.lower
  }
    if(alternative == "less"){
    p <- pnorm(z, lower.tail=TRUE)
  }
  if(alternative == "greater"){
    p <- pnorm(z, lower.tail=FALSE)
  }
   if(is.null(p2) | is.null(n2)){
     alpha <- conf.level
     lower.ci <- p1 - qnorm((1-alpha)/2) * sqrt(p1 * (1 - p1)/n1)
     upper.ci <- p1 + qnorm((1-alpha)/2) * sqrt(p1 * (1 - p1)/n1)
   } else {
      lower.ci <- (p2 - p1) - 1.96 * sqrt(((p1 * (1 - p1))/n1) + ((p2 * (1 - p2))/n2)) # Assuming a normal distribution, we can state that 95% of the sample mean would lie within 1.96 SEs above or below the popualtion mean, since 1.96 is the 2-sides 5% point of the standard normal distribution.
      upper.ci <- (p2 - p1) + 1.96 * sqrt(((p1 * (1 - p1))/n1) + ((p2 * (1 - p2))/n2))
    }
  outcome1 <- list(z, p, lower.ci, upper.ci)
  names(outcome1) <- c("z", "p", "lower.ci", "upper.ci")
  return(outcome1)
}
```

#####Using an example from class (one-sample Z-test):

```{r}
v1 <- c(0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1)
```

```{r}
one_sampleZtest <- z.prop.test(p1 = 0.6, n1 = 30, p0 = 0.8, conf.level = .95, alternative = "less")
one_sampleZtest
```

#####Using an example from class (two-sample Z-test):

```{r}
v1 <- c(1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0)
v2 <- c(1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1)
```

```{r}
two_sampleZtest <- z.prop.test(p1 = mean(v1), n1 = length(v1),p2 = mean(v2), n2 = length(v2), p0 = 0, conf.level = .95, alternative = "two.sided")
two_sampleZtest
```

<span style="color:blue">***PROBLEM-02:***</span>

The comparative primate dataset we have used from Kamilar and Cooper has in it a large number of variables related to life history and body size. For this exercise, the end aim is to fit a simple linear regression model to predict longevity (MaxLongevity_m) measured in months from species’ brain size (Brain_Size_Species_Mean) measured in grams. Do the following for both longevity~brain size and log(longevity)~log(brain size).

```{r}
library(readr)
library(tidyverse)
library(curl)
f <- curl("https://raw.githubusercontent.com/difiore/ADA-2019/master/KamilarAndCooperData.csv")
d <- read.csv(f, header = TRUE, stringsAsFactors = FALSE)
names(d)
```

a) Fit the regression model and, using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the data. Append the fitted model equation to your plot (HINT: use the function geom_text()).

####LONGEVITY ~ BRAIN SIZE
 
```{r}
library(ggplot2)
Longevity <- c(d$MaxLongevity_m)
BrainSize <- c(d$Brain_Size_Species_Mean)
m <- lm(data=d, Longevity ~ BrainSize)
summary(m)
m$coefficients
```

```{r}
g <- ggplot(data=d, aes(x=BrainSize, y=Longevity))
g <- g + labs(subtitle="Longevity ~ Brain Size")
g <- g + geom_point()
g <- g + geom_smooth(method = "lm", formula = y ~ x)
g <- g + geom_text(x=350, y=300,  label="y=1.218x + 248.952", color="blue")
g
```

####Log(LONGEVITY) ~ Log(BRAIN SIZE)

```{r}
LogLon <- log(Longevity)
LogBraSize <- log(BrainSize)
m1 <- lm(data=d, LogLon ~ LogBraSize)
summary(m1)
m1$coefficients
```

```{r}
g <- ggplot(data=d, aes(x=LogBraSize, y=LogLon))
g <- g + labs(subtitle="Log(Longevity) ~ Log(Brain Size)", x="Log(Brain Size)", y="Log(Longevity)")
g <- g + geom_point()
g <- g + geom_smooth(method = "lm", formula = y ~ x)
g <- g + geom_text(x=1.7, y=6.3,  label="y=0.2341x + 4.8790", color="blue")
g
```

b) Identify and interpret the point estimate of the slope (β1), as well as the outcome of the test associated with the hypotheses H0: β1=0; HA: β1≠0. Also, find a 90% CI for the slope (β1) parameter.

####LONGEVITY ~ BRAIN SIZE

`B1 <- 1.218`

`B0 <- 248.952`

<span style="color:green">***Interpretation:***</span>
B1 is not equal to zero (0) meaning that we reject the null hypothesis, supporting an effect of Brain Size on Longevity. For every 1 unit of change in Brain Size (x), we expected a 1.218 change in Longevity (as Brain Size increases, Longevity increases).

```{r}
alpha <- 0.1
ci_lm <- confint(m, level = 1 - alpha)
ci_lm
```

####Log(LONGEVITY) ~ Log(BRAIN SIZE)

`B1 <- 0.2341`

`B0 <- 4.8790`

<span style="color:green">***Interpretation:***</span>
B1 is not equal to zero (0), which means that we reject the null hypothesis, supporting an effect of Log(Brain Size) on Log(Longevity). For every 1 unit of change in Log(Brain Size) (x), we expected a 0.2341 change in Log(Longevity) (as Brain Size increases, Longevity increases).

```{r}
alpha <- 0.1
ci_lm1 <- confint(m1, level = 1 - alpha)
ci_lm1
```

c) Using your model, add lines for the 90% confidence and prediction interval bands on the plot, and add a legend to differentiate between the lines.

####LONGEVITY ~ BRAIN SIZE
```{r}
m <- lm(data=d, Longevity ~ BrainSize)
Longevity_hat <- predict(m, newdata = data.frame(BrainSize = d$Brain_Size_Species_Mean))
df <- data.frame(cbind(d$Brain_Size_Species_Mean, d$MaxLongevity_m, Longevity_hat))
names(df) <- c("x", "y", "yhat")
head(df)
```

```{r}
ci <- predict(m, newdata = data.frame(BrainSize = d$Brain_Size_Species_Mean), interval = "confidence", level = 0.90)  # for a vector of values
head(ci)
```

```{r}
pi <- predict(m, newdata = data.frame(BrainSize = d$Brain_Size_Species_Mean), interval = "prediction", level = 0.90)  # for a vector of values
head(pi)
```

```{r}
dff <- cbind(df, ci, pi)
names(dff) <- c("x", "y", "yhat", "CIfit", "CIlwr", "CIupr", "PIfit", "PIlwr", 
    "PIupr")
head(dff)
```

```{r}
g <- ggplot(data = dff, aes(x = BrainSize, y = Longevity))
g <- g + geom_point(alpha = 0.5)
g <- g + geom_line(aes(x = x, y = CIfit, colour = "black"))
g <- g + geom_line(aes(x = x, y = CIlwr, colour = "blue"))
g <- g + geom_line(aes(x = x, y = CIupr, colour = "blue"))
g <- g + geom_line(data = dff, aes(x = x, y = PIlwr, colour = "red"))
g <- g + geom_line(data = dff, aes(x = x, y = PIupr, colour = "red"))
g <- g + scale_color_identity(name=" ", breaks=c("black", "blue", "red"), labels=c("Reggresion line", "90% confidence intervals", "90% predicted intervals"), guide="legend")
g
```

####Log(LONGEVITY) ~ Log(BRAIN SIZE)

```{r}
m1 <- lm(data=d, LogLon ~ LogBraSize)
logData <- data.frame(LogLon, LogBraSize)
LogLon_hat <- predict(m1, newdata = data.frame(LogBrainSize = logData$LogBraSize))
df1 <- data.frame(cbind(logData$LogBraSize, logData$LogLon, LogLon_hat))
names(df1) <- c("x", "y", "yhat")
head(df1)
```

```{r}
ci1 <- predict(m1, newdata = data.frame(LogBrainSize = logData$LogBraSize), interval = "confidence", level = 0.90)  # for a vector of values
head(ci1)
```

```{r}
pi1 <- predict(m1, newdata = data.frame(LogBrainSize = logData$LogBraSize), interval = "prediction", level = 0.90)  # for a vector of values
head(pi1)
```

```{r}
dfflog <- cbind(df1, ci1, pi1)
names(dfflog) <- c("x", "y", "yhat", "CIfit", "CIlwr", "CIupr", "PIfit", "PIlwr", 
    "PIupr")
head(dfflog)
```

```{r}
g <- ggplot(data = dfflog, aes(x = LogBraSize, y = LogLon))
g <- g + geom_point(alpha = 0.5)
g <- g + geom_line(aes(x = x, y = CIfit, colour = "black"))
g <- g + geom_line(aes(x = x, y = CIlwr, colour = "blue"))
g <- g + geom_line(aes(x = x, y = CIupr, colour = "blue"))
g <- g + geom_line(data = dfflog, aes(x = x, y = PIlwr, colour = "red"))
g <- g + geom_line(data = dfflog, aes(x = x, y = PIupr, colour = "red"))
g <- g + scale_color_identity(name=" ", breaks=c("black", "blue", "red"), labels=c("Reggresion line", "90% confidence intervals", "90% predicted intervals"), guide="legend")
g
```

d) Produce a point estimate and associated 90% prediction interval for the longevity of a species whose brain weight is 800 gm.

####LONGEVITY ~ BRAIN SIZE
```{r}
plongevity <- predict(m, newdata = data.frame(BrainSize = 800), interval = "prediction", level = 0.90)
plongevity
```

####Log(LONGEVITY) ~ Log(BRAIN SIZE)
```{r}
plongevity1 <- predict(m1, newdata = data.frame(LogBraSize = 2.9030), interval = "prediction", level = 0.90)
plongevity1
```

d) Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?

<span style="color:green">***Answer:***</span>
Based on the model, the Longevity expected for a Brain Size of 800gr would be 1223.345. Is difficult to trust on this prediction because as Brain Size increases, the width of the confidence intervals increase as well, meaning that this prediction would be non-well supported. Also, because we don't have any data above Brain Size of 500gr is difficult to conclude that the association between the two variables (Longevity & Brain Size) will continue having a positive correlation.     

e) Looking at your two models, which do you think is better? Why?

####LONGEVITY ~ BRAIN SIZE
```{r}
g <- ggplot(data = df, aes(x = x, y = yhat))
g <- g + geom_point()
g <- g + geom_point(aes(x = x, y = y), colour = "red")
g <- g + geom_segment(aes(x = x, y = yhat, xend = x, yend = y))
g
```

```{r}
residuals <- resid(m)
dfL_BS <- data.frame(d$MaxLongevity_m, d$Brain_Size_Species_Mean)
dfL_BSnNA <- na.omit(dfL_BS)
residuals <- resid(m)
residualsdf <- data.frame(residuals)
resi_plot <- plot(dfL_BSnNA$d.Brain_Size_Species_Mean, residualsdf$residuals, ylab="Residuals", xlab="Brain Size", main="Longevity", abline(0, 0))
```


####Log(LONGEVITY) ~ Log(BRAIN SIZE)
```{r}
g <- ggplot(data = df1, aes(x = x, y = yhat))
g <- g + geom_point()
g <- g + geom_point(aes(x = x, y = y), colour = "red")
g <- g + geom_segment(aes(x = x, y = yhat, xend = x, yend = y))
g
```

```{r}
residuals1 <- resid(m1)
dfL_BS1 <- data.frame(LogLon, LogBraSize)
dfL_BSnNA1 <- na.omit(dfL_BS1)
residualsdf1 <- data.frame(residuals1)
resi_plot1 <- plot(dfL_BSnNA1$LogBraSize, residualsdf1$residuals1, ylab="Residuals", xlab="Brain Size", main="Longevity", abline(0, 0))
```

I think that between both models, the log-model is better because 57.84% of the variance in Longevity is explained by Brain Size. In contrast, in the non-log-model, only 49.28% of the variance in Longevity is explained by Brain Size. Additionally, the log-model present a random pattern in the residuals, which is not present on the residuals of the non-log-model.
